{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "- DETR의 Transformer architecture를 기반으로 함.\n",
    "- 차이점은 아래와 같다.\n",
    "- 1. positional encoding이 Multi-Head Attention에 직접 전달\n",
    "- 2. Encoder의 마지막 LayerNorm이 제거되었다.\n",
    "- 3. Decoder가 여러 Layer의 출력을 모두 반환할 수 있도록 변경\n",
    "- 4. 이전 Frame 정보를 활용할 수 있도록 Transformer architecture 확장 (track_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Transformer`\n",
    "- CNN에서 추출한 Feature을 Transforemr Encoder에서 처리\n",
    "- Decoder에서 Query 기반 객체 예측 및 Tracking-by-Attention 적용\n",
    "- 이전 Frame 정보를 활용해 Tracking-by-Attention 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False,\n",
    "                 return_intermediate_dec=False,\n",
    "                 track_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_model: Transformer의 차원 수 (512)\n",
    "        # nhead: Multi-Head Attention의 헤드 수 (8개)\n",
    "        # num_encoder_layers: Encoder 층 개수 (6개)\n",
    "        # num_decoder_layers: Decoder 층 개수 (6개)\n",
    "        # dim_feedforward: Feedforward 네트워크의 차원 수 (2048)\n",
    "        # dropout: Dropout 비율 (0.1)\n",
    "        # activation: 활성화 함수 선택 (ReLU)\n",
    "        # normalize_before: Layer Normalization을 수행하는 위치 선택\n",
    "        # return_intermediate_dec: Decoder의 중간 출력을 반환할지 여부\n",
    "        # track_attention: 이전 프레임 정보를 활용하는 Attention 적용 여부\n",
    "        # => Object Tracking을 위해 track_attention을 추가하여 이전 Frame 정보를 활용용\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,\n",
    "                                                dropout, activation, normalize_before)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = TransformerDecoder(\n",
    "            decoder_layer, encoder_layer, num_decoder_layers, decoder_norm,\n",
    "            return_intermediate=return_intermediate_dec,\n",
    "            track_attention=track_attention)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # normalize_before: True일 경우 LayerNorm을 Encoder의 입력으로 적용 (Pre-Norm 방식)\n",
    "        # track_attention=True이면 이전 프레임 정보를 활용하는 추가 Attention Layer를 사용\n",
    "        # => \"Tracking-by-Attention\"을 구현하기 위해 Transformer 구조를 확장\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # => 모든 가중치를 Xavier Uniform 초기화 방식으로 설정\n",
    "\n",
    "    def forward(self, src, mask, query_embed, pos_embed, tgt=None, prev_frame=None):\n",
    "        # flatten NxCxHxW to HWxNxC\n",
    "        bs, c, h, w = src.shape\n",
    "        src = src.flatten(2).permute(2, 0, 1)\n",
    "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
    "        mask = mask.flatten(1)\n",
    "\n",
    "        # src: CNN 백본에서 추출한 특징 맵\n",
    "        # mask: 패딩(mask) 정보\n",
    "        # query_embed: 객체를 예측하기 위한 Query Embedding\n",
    "        # pos_embed: Transformer의 위치 정보를 유지하는 Position Embedding\n",
    "        # tgt: Decoder의 초기 입력 값 (객체 위치 예측을 위한 Query로 활용됨)\n",
    "        # prev_frame: 이전 프레임 정보를 저장하여 Tracking 수행 (선택적)\n",
    "        # => N x C x H x W → HW x N x C 형태로 변환하여 Transformer에 입력할 수 있도록 조정\n",
    "\n",
    "        if tgt is None:\n",
    "            tgt = torch.zeros_like(query_embed)\n",
    "        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)\n",
    "        \n",
    "        # memory: Transformer Encoder에서 추출된 최종 표현 (Feature Memory)\n",
    "        # => Transformer Encoder는 CNN의 Feature를 입력받아 객체의 시각적 정보를 학습하는 역할\n",
    "\n",
    "        memory_prev_frame = None\n",
    "        if prev_frame is not None:\n",
    "            src_prev_frame = prev_frame['src'].flatten(2).permute(2, 0, 1)\n",
    "            pos_embed_prev_frame = prev_frame['pos'].flatten(2).permute(2, 0, 1)\n",
    "            mask_prev_frame = prev_frame['mask'].flatten(1)\n",
    "\n",
    "            memory_prev_frame = self.encoder(\n",
    "                src_prev_frame, src_key_padding_mask=mask_prev_frame, pos=pos_embed_prev_frame)\n",
    "\n",
    "            prev_frame['memory'] = memory_prev_frame\n",
    "            prev_frame['memory_key_padding_mask'] = mask_prev_frame\n",
    "            prev_frame['pos'] = pos_embed_prev_frame\n",
    "\n",
    "        # prev_frame이 존재하면, 이전 프레임의 특징을 Encoder에 다시 전달하여 Memory 저장\n",
    "        # memory_prev_frame을 Transformer Decoder에 전달하여 Tracking-by-Attention 구현\n",
    "        # => 즉, 이전 Frame의 정보를 활용하는 것이 TrackFormer의 중요한 차별점\n",
    "\n",
    "        hs, hs_without_norm = self.decoder(tgt, memory, memory_key_padding_mask=mask,\n",
    "                                           pos=pos_embed, query_pos=query_embed,\n",
    "                                           prev_frame=prev_frame)\n",
    "\n",
    "        # memory: Transformer Encoder에서 나온 Feature Memory\n",
    "        # query_embed: 객체를 탐색할 Query\n",
    "        # prev_frame: 이전 Frame 정보를 활용하여 Tracking 수행\n",
    "        # => Decoder는 현재 Frame의 Feature와 Query를 사용하여 객체를 탐지하고, 이전 Frame 정보를 활용하여 Tracking을 수행\n",
    "\n",
    "        return (hs.transpose(1, 2),\n",
    "            hs_without_norm.transpose(1, 2),\n",
    "            memory.permute(1, 2, 0).view(bs, c, h, w))\n",
    "    \n",
    "        # hs: Decoder의 최종 출력 (객체 탐지 결과)\n",
    "        # memory: Transformer Encoder에서 나온 Feature Memory\n",
    "        # memory.permute(1, 2, 0).view(bs, c, h, w): CNN Feature Map 크기로 변환하여 반환\n",
    "        # => 최종적으로 Object Detection 결과를 출력력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerEncoder`\n",
    "- CNN에서 추출한 Feature를 Transformer 입력으로 변환\n",
    "- Encoder를 통해 객체 간의 관계를 학습\n",
    "- Encoder가 Frame별 특징을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    # normalize_before=True이면 LayerNorm을 Encoder의 입력에 적용 (Pre-Norm)\n",
    "    # normalize_before=False이면 LayerNorm을 Encoder의 출력에 적용 (Post-Norm)   \n",
    "    # => 여러개의 Transformer Encoder Layer를 쌓아서 입력 특징을 처리리\n",
    "\n",
    "    def forward(self, src,\n",
    "                mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        output = src\n",
    "        # CNN 백본에서 추출한 특징을 Transformer Encoder의 입력으로 전달\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask=mask,\n",
    "                           src_key_padding_mask=src_key_padding_mask, pos=pos)\n",
    "        # Multi-Head Attention을 통해 입력 특징 간의 관계 학습\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    # src: CNN 백본에서 추출된 특징 맵 (Flatten된 형태)\n",
    "    # mask: Attention Mask (필요한 경우만 사용)\n",
    "    # src_key_padding_mask: 입력에서 패딩된 부분을 무시하기 위한 Mask\n",
    "    # pos: 위치 인코딩 (Positional Encoding)\n",
    "    # => Encoder는 CNN 특징을 Transformer 구조로 변환하여 객체 간 관계를 학습하고, 위치 정보를 유지하기 위해 Positional Encoding을 Attention에 직접 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerDecoder`\n",
    "- Query 기반 객체 탐지 및 Tracking-by-Attention 수행\n",
    "- 이전 Frame 정보를 활용하여 객체 연속성을 유지 (Multi-Frame Attention)\n",
    "- Query 일부를 Tracking Query로 변형하여 Frame 간 연관성을 학습\n",
    "- 객체 추적을 위한 Query Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, encoder_layer, num_layers,\n",
    "                 norm=None, return_intermediate=False, track_attention=False):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.return_intermediate = return_intermediate\n",
    "\n",
    "        self.track_attention = track_attention\n",
    "        if self.track_attention:\n",
    "            self.layers_track_attention = _get_clones(encoder_layer, num_layers)\n",
    "\n",
    "    # return_intermediate: Decoder의 중간 출력을 반환할지 여부\n",
    "    # track_attention: 이전 Frame 정보를 활용할지 여부 \n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None,\n",
    "                prev_frame: Optional[dict] = None):\n",
    "        \n",
    "                # tgt: Decoder의 초기 입력 값 (Query Embeddings)\n",
    "                # memory: Transformer Encoder의 출력 (CNN 특징을 Transformer로 변환한 Feature Memory)\n",
    "                # tgt_mask: Decoder의 Self-Attention Mask\n",
    "                # memory_mask: Encoder-Decoder Attention Mask\n",
    "                # tgt_key_padding_mask: Query에서 패딩된 부분을 무시하기 위한 Mask\n",
    "                # memory_key_padding_mask: Memory에서 패딩된 부분을 무시하기 위한 Mask\n",
    "                # pos: Transformer Encoder의 위치 인코딩 (Positional Encoding)\n",
    "                # query_pos: Query 위치 인코딩\n",
    "                # prev_frame: 이전 Frame의 정보가 포함된 딕셔너리 (Tracking-by-Attention 수행 시 사용)\n",
    "                        \n",
    "        output = tgt\n",
    "\n",
    "        intermediate = []\n",
    "\n",
    "        if self.track_attention:\n",
    "            track_query_pos = query_pos[:-100].clone()\n",
    "            query_pos[:-100] = 0.0\n",
    "\n",
    "        # Query Manipulation\n",
    "        # self.track_attention=True이면, Query 중 일부를 이전 Frame 객체 정보를 추적하는 Query로 활용\n",
    "        # query_pos[:-100] = 0.0 → Tracking Query가 아닌 부분은 0으로 초기화하여 Tracking Attention에 영향을 주지 않도록 설정\n",
    "        # => 즉, Query 중 일부는 현재 Frame에서 객체를 탐색하고, 일부는 이전 Frame 객체를 추적하는 데 사용\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.track_attention:\n",
    "                track_output = output[:-100].clone()\n",
    "\n",
    "                track_output = self.layers_track_attention[i](\n",
    "                    track_output,\n",
    "                    src_mask=tgt_mask,\n",
    "                    src_key_padding_mask=tgt_key_padding_mask,\n",
    "                    pos=track_query_pos)\n",
    "\n",
    "                output = torch.cat([track_output, output[-100:]])\n",
    "\n",
    "        # track_output = output[:-100].clone() → 현재 Query 중 Tracking Query 부분만 추출\n",
    "        # track_output = self.layers_track_attention[i](...) → 이전 Frame의 Encoder Feature와 Tracking Query 간의 Attention 수행\n",
    "        # output = torch.cat([track_output, output[-100:]]) → 이전 Frame 객체 정보와 현재 Query 정보를 결합하여 Decoder에 전달\n",
    "        # => Multi-Frame Attention 적용\n",
    "\n",
    "            output = layer(output, memory, tgt_mask=tgt_mask,\n",
    "                           memory_mask=memory_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                           memory_key_padding_mask=memory_key_padding_mask,\n",
    "                           pos=pos, query_pos=query_pos)\n",
    "            if self.return_intermediate:\n",
    "                intermediate.append(output)\n",
    "\n",
    "            # => Query를 기반으로 객체를 탐지하고, 이전 Frame 정보를 활용하여 Tracking을 수행\n",
    "\n",
    "        if self.return_intermediate:\n",
    "            output = torch.stack(intermediate)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            return self.norm(output), output\n",
    "        return output, output\n",
    "    \n",
    "        # return_intermediate=True이면, 모든 Decoder Layer의 출력을 반환\n",
    "        # self.norm is not None이면 Layer Normalization 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerEncoderLayer`\n",
    "- TrackFormer의 Transformer Encoder의 한 계층을 구성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # self.self_attn: Multi-Head Self-Attention Layer → 입력 특징들 간의 관계를 학습습\n",
    "        # self.linear1 → self.linear2: Feedforward Network (FFN)\n",
    "        # => Self-Attention을 통해 객체 간 관계를 학습한 후, FFN을 통해 특징을 강화\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    # pos(위치 인코딩)가 주어지면, 입력 Tensor에 위치 인코딩을 추가하여 Transformer가 공간적 정보를 인식할 수 있도록 함\n",
    "    # => TrackFormer의 Transformer Encoder는 위치 정보를 유지하기 위해 Positional Encoding을 Self-Attention에 직접 전달\n",
    "\n",
    "    def forward_post(self,\n",
    "                     src,\n",
    "                     src_mask: Optional[Tensor] = None,\n",
    "                     src_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None):\n",
    "        q = k = self.with_pos_embed(src, pos)\n",
    "        # 입력에 pos 추가\n",
    "        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        # MHS 적용\n",
    "        src = src + self.dropout1(src2)\n",
    "        # Residual Connection 적용\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        # FFN 적용\n",
    "        src = src + self.dropout2(src2)\n",
    "        # Residual Connection 적용\n",
    "        src = self.norm2(src)\n",
    "        # Layer Normalization 적용\n",
    "        return src\n",
    "        # => Self-Attention 수행\n",
    "\n",
    "    def forward_pre(self, src,\n",
    "                    src_mask: Optional[Tensor] = None,\n",
    "                    src_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None):\n",
    "        src2 = self.norm1(src)\n",
    "        # 입력에 먼저 Layer Normalization 적용\n",
    "        q = k = self.with_pos_embed(src2, pos)\n",
    "        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        return src\n",
    "    \n",
    "    # forward_post()와 거의 동일하지만, Layer Normalization을 먼저 적용하는 차이점이 있다.\n",
    "    # => Pre-Norm과 Post-Norm을 선택할 수 있도록 구성\n",
    "\n",
    "    def forward(self, src,\n",
    "                src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)\n",
    "        return self.forward_post(src, src_mask, src_key_padding_mask, pos)\n",
    "    \n",
    "    # normalize_before=True이면 forward_pre() 실행 (Pre-Norm)\n",
    "    # normalize_before=False이면 forward_post() 실행 (Post-Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TransformerDecoderLayer`\n",
    "- TrackFormer의 Transformer Decoder의 한 계층을 구성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", normalize_before=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        # self.self_attn: Multi-Head Self-Attention Layer → Query간의 관계를 학습\n",
    "        # self.multihead_attn: Cross-Attention Layer → Query와 Encoder Memory간의 관계를 학습\n",
    "        # self.linear1 → self.linear2: Feedforward Network (FFN)\n",
    "        # => Query 기반 객체 탐색을 Self-Attention을 통해 수행하고, Cross-Attention을 통해 CNN Feature와 연관성을 학습한 후, FFN을 통해 특징을 강화\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n",
    "        return tensor if pos is None else tensor + pos\n",
    "\n",
    "    # pos(위치 인코딩)가 주어지면, 입력 Tensor에 위치 인코딩을 추가하여 Transformer가 공간적 정보를 인식할 수 있도록 함\n",
    "    # => Transformer Decoder는 Query에 위치 정보를 추가하여 Tracking-by-Attention을 수행\n",
    "\n",
    "    def forward_post(self, tgt, memory,\n",
    "                     tgt_mask: Optional[Tensor] = None,\n",
    "                     memory_mask: Optional[Tensor] = None,\n",
    "                     tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                     memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                     pos: Optional[Tensor] = None,\n",
    "                     query_pos: Optional[Tensor] = None):\n",
    "        \n",
    "        q = k = self.with_pos_embed(tgt, query_pos)\n",
    "        # Query에 pos 추가\n",
    "        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        # MHS 적용\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        # Residual Connection 적용\n",
    "        tgt = self.norm1(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        # => Self-Attention 수행\n",
    "\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        # Query에 pos 추가 / CNN Feature에 pos 추가 / Query와 CNN Feature간 관계 학습습\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        # Residual Connection 적용\n",
    "        tgt = self.norm2(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        # => Cross-Attention 수행\n",
    "        \n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        return tgt\n",
    "        # FFN 수행\n",
    "\n",
    "    def forward_pre(self, tgt, memory,\n",
    "                    tgt_mask: Optional[Tensor] = None,\n",
    "                    memory_mask: Optional[Tensor] = None,\n",
    "                    tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                    memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                    pos: Optional[Tensor] = None,\n",
    "                    query_pos: Optional[Tensor] = None):\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        q = k = self.with_pos_embed(tgt2, query_pos)\n",
    "        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),\n",
    "                                   key=self.with_pos_embed(memory, pos),\n",
    "                                   value=memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        # Layer Normalization 적용\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    # forward_post()와 거의 동일하지만, Layer Normalization을 먼저 적용하는 차이점이 있다.\n",
    "    # => Pre-Norm과 Post-Norm을 선택할 수 있도록 구성\n",
    "\n",
    "    def forward(self, tgt, memory,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None,\n",
    "                pos: Optional[Tensor] = None,\n",
    "                query_pos: Optional[Tensor] = None):\n",
    "        if self.normalize_before:\n",
    "            return self.forward_pre(tgt, memory, tgt_mask, memory_mask,\n",
    "                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "        return self.forward_post(tgt, memory, tgt_mask, memory_mask,\n",
    "                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)\n",
    "    \n",
    "    # normalize_before=True이면 forward_pre() 실행 (Pre-Norm)\n",
    "    # normalize_before=False이면 forward_post() 실행 (Post-Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_get_clones`\n",
    "- 특정 Module을 N개 복사하여 리스트로 반환\n",
    "\n",
    "`_get_activation_fn(activation)`\n",
    "- 문자열을 입력으로 받아 해당하는 Activation Function(ReLU,GELU,GLU)을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "# Transformer의 Encoder와 Decoder가 여러 층으로 구성될 수 있도록 하는 유틸리티 함수\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "# Transformer의 Feedforward Network(FFN)에서 사용할 활성화 함수를 선택할 수 있도록 하는 유틸리티 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bulid_transformer`\n",
    "- TrackFormer에서 사용할 Transformer를 생성하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(args):\n",
    "    return Transformer(\n",
    "        d_model=args.hidden_dim,\n",
    "        dropout=args.dropout,\n",
    "        nhead=args.nheads,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        num_encoder_layers=args.enc_layers,\n",
    "        num_decoder_layers=args.dec_layers,\n",
    "        normalize_before=args.pre_norm,\n",
    "        return_intermediate_dec=True,\n",
    "        track_attention=args.track_attention\n",
    "    )\n",
    "# Transformer 모델 생성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
